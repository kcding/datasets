{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bsoup01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM23rq+9dXUT7nlYKQMUdvW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcding/datasets/blob/master/bsoup01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tob5Po0F4io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a811b06c-c822-4799-85db-9da1fa4ded22"
      },
      "source": [
        "# Scraping Python.org with Requests and Beautiful Soup\n",
        "\n",
        "# Book: Python Web Scraping Cookbook (Packt)\n",
        "# Chapter 1: 01_events_with_requests.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Getting ready...\n",
        "\n",
        "# In this recipe, we will scrape the upcoming Python events \n",
        "# from https://www.python.org/events/pythonevents. \n",
        "\n",
        "def get_upcoming_events(url):\n",
        "\n",
        "    # Use requests to make a GET HTTP request for the following \n",
        "    # url: https://www.python.org/events/python-events/ by making \n",
        "    # a GET request:\n",
        "    req = requests.get(url)\n",
        "\n",
        "    # The above requests command downloaded the page content but it is stored \n",
        "    # in our requests object req. We can retrieve the content using the .text \n",
        "    # property. \n",
        "    soup = BeautifulSoup(req.text, 'lxml')\n",
        "\n",
        "    # Now we tell Beautiful Soup to find the main <ul> tag for the recent \n",
        "    # events, and then to get all the <li> tags below it.\n",
        "    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
        "\n",
        "\n",
        "    # And finally we can loop through each of the <li> elements, \n",
        "    # extracting the event details, and print each to the console:\n",
        "    for event in events:\n",
        "        event_details = dict()\n",
        "        event_details['name'] = event.find('h3').find(\"a\").text\n",
        "        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
        "        event_details['time'] = event.find('time').text\n",
        "        print(event_details)\n",
        "\n",
        "get_upcoming_events('https://www.python.org/events/python-events/')\n",
        "\n",
        "# How it works...\n",
        "\n",
        "# We will dive into details of both Requests and Beautiful Soup in the next \n",
        "# chapter, but for now let's just summarize a few key points about how this \n",
        "# works. \n",
        "\n",
        "# The following important points about Requests:\n",
        "\n",
        "# - Requests is used to execute HTTP requests. We used it to make a GET verb\n",
        "#   request of the URL for the events page.\n",
        "# - The Requests object holds the results of the request. This is not only \n",
        "#   the page content, but also many other items about the result such as HTTP \n",
        "#   status codes and headers.\n",
        "# - Requests is used only to get the page, it does not do an parsing.\n",
        "\n",
        "# We use Beautiful Soup to do the parsing of the HTML and also the finding of \n",
        "# content within the HTML. \n",
        "\n",
        "# We used the power of Beautiful Soup to:\n",
        "# - Find the <ul> element representing the section, which is found by looking \n",
        "#   for a <ul> with the a class attribute that has a value of list-recent-events.\n",
        "# - From that object, we find all the <li> elements.\n",
        "\n",
        "# Each of these <li> tags represent a different event. We iterate over each of \n",
        "# those making a dictionary from the event data found in child HTML tags:\n",
        "# - The name is extracted from the <a> tag that is a child of the <h3> tag\n",
        "# - The location is the text content of the <span> with a class of event-location\n",
        "# - And the time is extracted from the datetime attribute of the <time> tag."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'name': 'PyCon JP 2020', 'location': 'Tokyo, Japan', 'time': '28 Aug. – 29 Aug.  2020'}\n",
            "{'name': 'PyCon TW 2020', 'location': 'International Conference Hall ,No.1, University Road, Tainan City 701, Taiwan', 'time': '05 Sept. – 06 Sept.  2020'}\n",
            "{'name': 'PyCon SK 2020', 'location': 'Bratislava, Slovakia', 'time': '11 Sept. – 13 Sept.  2020'}\n",
            "{'name': 'DjangoCon Europe 2020', 'location': 'Porto, Portugal', 'time': '16 Sept. – 20 Sept.  2020'}\n",
            "{'name': 'PyCon APAC 2020', 'location': 'Kota Kinabalu, Sabah, Malaysia', 'time': '19 Sept. – 20 Sept.  2020'}\n",
            "{'name': 'DragonPy 2020', 'location': 'Ljubljana, Slovenia', 'time': '19 Sept. – 20 Sept.  2020'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}